#!/usr/bin/env python3
"""
DLoRAL å†…å­˜é…ç½®åŠ©æ‰‹
æ ¹æ®GPUæ˜¾å­˜å¤§å°æ¨èåˆé€‚çš„å¤„ç†å‚æ•°
"""

import torch

def get_gpu_memory_info():
    """è·å–GPUå†…å­˜ä¿¡æ¯"""
    if not torch.cuda.is_available():
        return None, "CUDAä¸å¯ç”¨"
    
    gpu_count = torch.cuda.device_count()
    memory_info = []
    
    for i in range(gpu_count):
        total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB
        allocated_memory = torch.cuda.memory_allocated(i) / (1024**3)  # GB
        free_memory = total_memory - allocated_memory
        
        memory_info.append({
            'device_id': i,
            'name': torch.cuda.get_device_name(i),
            'total_gb': total_memory,
            'allocated_gb': allocated_memory,
            'free_gb': free_memory
        })
    
    return memory_info, None

def recommend_parameters(gpu_memory_gb):
    """æ ¹æ®GPUæ˜¾å­˜æ¨èå‚æ•°"""
    if gpu_memory_gb < 8:
        return {
            'vae_encoder_tiled_size': 512,
            'process_size': 256,
            'upscale': 2,
            'description': 'ä½å†…å­˜æ¨¡å¼ - é€‚åˆ8GBä»¥ä¸‹æ˜¾å­˜'
        }
    elif gpu_memory_gb < 12:
        return {
            'vae_encoder_tiled_size': 1024,
            'process_size': 384,
            'upscale': 2,
            'description': 'å¹³è¡¡æ¨¡å¼ - é€‚åˆ8-12GBæ˜¾å­˜'
        }
    elif gpu_memory_gb < 16:
        return {
            'vae_encoder_tiled_size': 1024,
            'process_size': 512,
            'upscale': 4,
            'description': 'æ ‡å‡†æ¨¡å¼ - é€‚åˆ12-16GBæ˜¾å­˜'
        }
    elif gpu_memory_gb < 20:
        return {
            'vae_encoder_tiled_size': 2048,
            'process_size': 512,
            'upscale': 4,
            'description': 'é«˜æ€§èƒ½æ¨¡å¼ - é€‚åˆ16-20GBæ˜¾å­˜'
        }
    else:
        return {
            'vae_encoder_tiled_size': 4096,
            'process_size': 512,
            'upscale': 4,
            'description': 'æœ€é«˜æ€§èƒ½æ¨¡å¼ - é€‚åˆ20GB+æ˜¾å­˜'
        }

def print_memory_analysis():
    """æ‰“å°å†…å­˜åˆ†ææŠ¥å‘Š"""
    print("ğŸ” DLoRAL å†…å­˜é…ç½®åˆ†æ")
    print("=" * 50)
    
    memory_info, error = get_gpu_memory_info()
    
    if error:
        print(f"âŒ {error}")
        return
    
    for gpu in memory_info:
        print(f"\nğŸ® GPU {gpu['device_id']}: {gpu['name']}")
        print(f"   æ€»æ˜¾å­˜: {gpu['total_gb']:.1f} GB")
        print(f"   å·²ä½¿ç”¨: {gpu['allocated_gb']:.1f} GB")
        print(f"   å¯ç”¨: {gpu['free_gb']:.1f} GB")
        
        # æ¨èå‚æ•°
        recommended = recommend_parameters(gpu['free_gb'])
        print(f"\nğŸ“‹ æ¨èé…ç½® ({recommended['description']}):")
        print(f"   VAEç¼–ç å™¨åˆ†å—å¤§å°: {recommended['vae_encoder_tiled_size']}")
        print(f"   å¤„ç†å°ºå¯¸: {recommended['process_size']}")
        print(f"   è¶…åˆ†å€æ•°: {recommended['upscale']}")
        
        # å†…å­˜ä½¿ç”¨ä¼°ç®—
        estimated_memory = estimate_memory_usage(recommended)
        print(f"\nğŸ’¾ é¢„ä¼°å†…å­˜ä½¿ç”¨: {estimated_memory:.1f} GB")
        
        if estimated_memory > gpu['free_gb']:
            print("âš ï¸  è­¦å‘Š: é¢„ä¼°å†…å­˜ä½¿ç”¨è¶…è¿‡å¯ç”¨æ˜¾å­˜ï¼Œå»ºè®®é™ä½å‚æ•°")
        else:
            print("âœ… å†…å­˜é…ç½®åˆç†")

def estimate_memory_usage(config):
    """ä¼°ç®—å†…å­˜ä½¿ç”¨é‡"""
    # ç®€åŒ–çš„å†…å­˜ä¼°ç®—æ¨¡å‹
    base_memory = 2.0  # åŸºç¡€æ¨¡å‹å†…å­˜
    vae_factor = config['vae_encoder_tiled_size'] / 1024.0
    process_factor = (config['process_size'] / 512.0) ** 2
    upscale_factor = config['upscale'] / 4.0
    
    estimated = base_memory * vae_factor * process_factor * upscale_factor
    return min(estimated, 20.0)  # æœ€å¤§ä¼°ç®—20GB

def get_optimal_config():
    """è·å–æœ€ä¼˜é…ç½®"""
    memory_info, error = get_gpu_memory_info()
    
    if error or not memory_info:
        # é»˜è®¤é…ç½®
        return {
            'vae_encoder_tiled_size': 1024,
            'process_size': 512,
            'upscale': 2
        }
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPUçš„å¯ç”¨å†…å­˜
    free_memory = memory_info[0]['free_gb']
    return recommend_parameters(free_memory)

if __name__ == "__main__":
    print_memory_analysis() 