import os
import sys
import time
import tempfile
import shutil
import gradio as gr
import torch
import cv2
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
import subprocess

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.getcwd())

# å…¨å±€å˜é‡
model_initialized = False

def initialize_models():
    """åˆå§‹åŒ–æ¨¡å‹"""
    global model_initialized
    
    try:
        print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„æ¨¡å‹åˆå§‹åŒ–ä»£ç 
        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åªæ˜¯æ¨¡æ‹Ÿåˆå§‹åŒ–è¿‡ç¨‹
        time.sleep(2)  # æ¨¡æ‹ŸåŠ è½½æ—¶é—´
        
        model_initialized = True
        print("æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼")
        return "âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼"
    except Exception as e:
        return f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}"

def extract_frames_simple(video_path, output_dir):
    """ç®€åŒ–ç‰ˆå¸§æå–"""
    video_capture = cv2.VideoCapture(video_path)
    
    frame_number = 0
    success, frame = video_capture.read()
    frame_images = []
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # é™åˆ¶å¤„ç†çš„å¸§æ•°ï¼Œé¿å…å¤„ç†æ—¶é—´è¿‡é•¿
    max_frames = 30
    
    # å¾ªç¯å¤„ç†å¸§
    while success and frame_number < max_frames:
        frame_filename = f"frame_{frame_number:04d}.png"
        frame_path = os.path.join(output_dir, frame_filename)
        cv2.imwrite(frame_path, frame)
        frame_images.append(frame_path)
        
        success, frame = video_capture.read()
        frame_number += 1
    
    video_capture.release()
    print(f"ä» {video_path} æå–äº† {len(frame_images)} å¸§")
    
    return frame_images

def process_video_simple(video_path, prompt="", align_method="adain", process_size=512, upscale=4):
    """ç®€åŒ–ç‰ˆè§†é¢‘å¤„ç†"""
    global model_initialized
    
    if not model_initialized:
        return None, "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆå§‹åŒ–æ¨¡å‹"
    
    if video_path is None:
        return None, "è¯·å…ˆä¸Šä¼ è§†é¢‘æ–‡ä»¶"
    
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            frames_dir = os.path.join(temp_dir, "frames")
            output_dir = os.path.join(temp_dir, "output")
            os.makedirs(frames_dir, exist_ok=True)
            os.makedirs(output_dir, exist_ok=True)
            
            # æå–å¸§
            frame_images = extract_frames_simple(video_path, frames_dir)
            
            if len(frame_images) == 0:
                return None, "æ— æ³•ä»è§†é¢‘ä¸­æå–å¸§"
            
            # ç®€åŒ–å¤„ç†ï¼šåªå¤„ç†å‰å‡ å¸§ä½œä¸ºæ¼”ç¤º
            processed_frames = []
            
            for i, frame_path in enumerate(frame_images[:5]):  # åªå¤„ç†å‰5å¸§
                # è¯»å–å›¾åƒ
                input_image = Image.open(frame_path).convert('RGB')
                
                # ç®€å•çš„å›¾åƒå¤„ç†ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…åº”è¯¥è°ƒç”¨DLoRALæ¨¡å‹ï¼‰
                # æ”¾å¤§å›¾åƒ
                new_width = input_image.width * upscale
                new_height = input_image.height * upscale
                processed_image = input_image.resize((new_width, new_height), Image.LANCZOS)
                
                # ä¿å­˜å¤„ç†åçš„å¸§
                output_frame_path = os.path.join(output_dir, f"processed_frame_{i:04d}.png")
                processed_image.save(output_frame_path)
                processed_frames.append(output_frame_path)
            
            # åˆ›å»ºè¾“å‡ºè§†é¢‘
            if len(processed_frames) > 0:
                # è¯»å–ç¬¬ä¸€å¸§è·å–å°ºå¯¸
                first_frame = cv2.imread(processed_frames[0])
                height, width, layers = first_frame.shape
                
                # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
                output_video_path = os.path.join(temp_dir, "output_video.mp4")
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                video_writer = cv2.VideoWriter(output_video_path, fourcc, 30.0, (width, height))
                
                # å†™å…¥å¸§
                for frame_path in processed_frames:
                    frame = cv2.imread(frame_path)
                    video_writer.write(frame)
                
                video_writer.release()
                
                # å¤åˆ¶åˆ°æœ€ç»ˆè¾“å‡ºè·¯å¾„
                final_output_path = os.path.join("results", f"output_{int(time.time())}.mp4")
                os.makedirs("results", exist_ok=True)
                shutil.copy2(output_video_path, final_output_path)
                
                return final_output_path, f"å¤„ç†å®Œæˆï¼å…±å¤„ç†äº† {len(processed_frames)} å¸§ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰"
            else:
                return None, "æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å¸§"
                
    except Exception as e:
        return None, f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"

def run_dloral_inference(video_path, prompt="", align_method="adain", process_size=512, upscale=4):
    """è¿è¡ŒDLoRALæ¨ç†"""
    try:
        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "src/test_DLoRAL.py",
            "--pretrained_model_path", "stabilityai/stable-diffusion-2-1-base",
            "--ram_ft_path", "preset/models/DAPE.pth",
            "--ram_path", "preset/models/ram_swin_large_14m.pth",
            "--merge_and_unload_lora", "False",
            "--process_size", str(process_size),
            "--pretrained_model_name_or_path", "stabilityai/stable-diffusion-2-1-base",
            "--vae_encoder_tiled_size", "4096",
            "--load_cfr",
            "--pretrained_path", "preset/models/checkpoints/model.pkl",
            "--stages", "1",
            "-i", video_path,
            "-o", "results"
        ]
        
        # è¿è¡Œå‘½ä»¤
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            # æŸ¥æ‰¾è¾“å‡ºè§†é¢‘æ–‡ä»¶
            output_files = []
            if os.path.exists("results"):
                for file in os.listdir("results"):
                    if file.endswith(".mp4"):
                        output_files.append(os.path.join("results", file))
            
            if output_files:
                return output_files[-1], "DLoRALå¤„ç†å®Œæˆï¼"
            else:
                return None, "å¤„ç†å®Œæˆä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶"
        else:
            return None, f"å¤„ç†å¤±è´¥: {result.stderr}"
            
    except Exception as e:
        return None, f"è¿è¡ŒDLoRALæ—¶å‡ºé”™: {str(e)}"

def create_web_interface():
    """åˆ›å»ºWebç•Œé¢"""
    
    # è‡ªå®šä¹‰CSSæ ·å¼
    css = """
    .gradio-container {
        max-width: 1200px !important;
    }
    .title {
        text-align: center;
        font-size: 2.5em;
        font-weight: bold;
        margin-bottom: 20px;
        color: #2c3e50;
    }
    .subtitle {
        text-align: center;
        font-size: 1.2em;
        color: #7f8c8d;
        margin-bottom: 30px;
    }
    """
    
    with gr.Blocks(css=css, title="DLoRAL è§†é¢‘è¶…åˆ†è¾¨ç‡") as demo:
        gr.HTML("""
        <div class="title">ğŸ¬ DLoRAL è§†é¢‘è¶…åˆ†è¾¨ç‡</div>
        <div class="subtitle">ä¸€æ­¥æ‰©æ•£ï¼šç»†èŠ‚ä¸°å¯Œä¸”æ—¶é—´ä¸€è‡´çš„è§†é¢‘è¶…åˆ†è¾¨ç‡</div>
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¤ è¾“å…¥è®¾ç½®")
                
                # æ¨¡å‹åˆå§‹åŒ–æŒ‰é’®
                init_button = gr.Button("ğŸš€ åˆå§‹åŒ–æ¨¡å‹", variant="primary", size="lg")
                init_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", value="æ¨¡å‹æœªåˆå§‹åŒ–", interactive=False)
                
                # è¾“å…¥è§†é¢‘
                input_video = gr.Video(label="ä¸Šä¼ è§†é¢‘æ–‡ä»¶", type="filepath")
                
                # å‚æ•°è®¾ç½®
                with gr.Accordion("âš™ï¸ é«˜çº§å‚æ•°", open=False):
                    prompt = gr.Textbox(label="æç¤ºè¯", value="", placeholder="è¾“å…¥é¢å¤–çš„æç¤ºè¯ï¼ˆå¯é€‰ï¼‰")
                    align_method = gr.Dropdown(
                        choices=["adain", "wavelet", "nofix"], 
                        value="adain", 
                        label="é¢œè‰²æ ¡æ­£æ–¹æ³•"
                    )
                    process_size = gr.Slider(
                        minimum=256, maximum=1024, value=512, step=64,
                        label="å¤„ç†å°ºå¯¸"
                    )
                    upscale = gr.Slider(
                        minimum=2, maximum=8, value=4, step=1,
                        label="è¶…åˆ†å€æ•°"
                    )
                
                # å¤„ç†æ¨¡å¼é€‰æ‹©
                process_mode = gr.Radio(
                    choices=["æ¼”ç¤ºæ¨¡å¼", "å®Œæ•´DLoRALæ¨¡å¼"],
                    value="æ¼”ç¤ºæ¨¡å¼",
                    label="å¤„ç†æ¨¡å¼"
                )
                
                # å¤„ç†æŒ‰é’®
                process_button = gr.Button("ğŸ¯ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¥ è¾“å‡ºç»“æœ")
                
                # è¾“å‡ºè§†é¢‘
                output_video = gr.Video(label="å¤„ç†åçš„è§†é¢‘")
                
                # å¤„ç†çŠ¶æ€
                status_text = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False)
                
                # å¤„ç†ä¿¡æ¯
                info_text = gr.Textbox(label="å¤„ç†ä¿¡æ¯", interactive=False, lines=3)
        
        # ä½¿ç”¨è¯´æ˜
        gr.Markdown("### ğŸ“‹ ä½¿ç”¨è¯´æ˜")
        gr.Markdown("""
        **æ¼”ç¤ºæ¨¡å¼**: å¿«é€Ÿæ¼”ç¤ºï¼Œåªå¤„ç†å‰å‡ å¸§ï¼Œé€‚åˆæµ‹è¯•ç•Œé¢åŠŸèƒ½
        **å®Œæ•´DLoRALæ¨¡å¼**: ä½¿ç”¨å®Œæ•´çš„DLoRALæ¨¡å‹å¤„ç†æ•´ä¸ªè§†é¢‘
        
        1. **åˆå§‹åŒ–æ¨¡å‹**: é¦–æ¬¡ä½¿ç”¨æˆ–é‡å¯åéœ€è¦å…ˆåˆå§‹åŒ–æ¨¡å‹
        2. **ä¸Šä¼ è§†é¢‘**: æ”¯æŒMP4æ ¼å¼çš„è§†é¢‘æ–‡ä»¶
        3. **é€‰æ‹©æ¨¡å¼**: é€‰æ‹©æ¼”ç¤ºæ¨¡å¼æˆ–å®Œæ•´å¤„ç†æ¨¡å¼
        4. **è®¾ç½®å‚æ•°**: å¯é€‰æ‹©è°ƒæ•´å¤„ç†å‚æ•°
        5. **å¼€å§‹å¤„ç†**: ç‚¹å‡»å¤„ç†æŒ‰é’®å¼€å§‹è§†é¢‘è¶…åˆ†è¾¨ç‡
        6. **ä¸‹è½½ç»“æœ**: å¤„ç†å®Œæˆåå¯ä¸‹è½½å¢å¼ºåçš„è§†é¢‘
        
        **æ³¨æ„äº‹é¡¹**:
        - å®Œæ•´æ¨¡å¼å¤„ç†æ—¶é—´è¾ƒé•¿ï¼Œå–å†³äºè§†é¢‘é•¿åº¦å’Œåˆ†è¾¨ç‡
        - å»ºè®®è§†é¢‘åˆ†è¾¨ç‡ä¸è¶…è¿‡1080p
        - ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUå†…å­˜
        - æ¼”ç¤ºæ¨¡å¼ä»…ç”¨äºæµ‹è¯•ç•Œé¢åŠŸèƒ½
        """)
        
        # äº‹ä»¶å¤„ç†
        def init_models():
            return initialize_models()
        
        def process_video(video_path, prompt, align_method, process_size, upscale, process_mode):
            if video_path is None:
                return None, "è¯·å…ˆä¸Šä¼ è§†é¢‘æ–‡ä»¶", "æœªå¤„ç†"
            
            if not model_initialized:
                return None, "è¯·å…ˆåˆå§‹åŒ–æ¨¡å‹", "æ¨¡å‹æœªåˆå§‹åŒ–"
            
            if process_mode == "æ¼”ç¤ºæ¨¡å¼":
                output_path, message = process_video_simple(
                    video_path, prompt, align_method, process_size, upscale
                )
            else:
                output_path, message = run_dloral_inference(
                    video_path, prompt, align_method, process_size, upscale
                )
            
            return output_path, message, f"å¤„ç†å®Œæˆ - {message}"
        
        # ç»‘å®šäº‹ä»¶
        init_button.click(
            fn=init_models,
            outputs=init_status
        )
        
        process_button.click(
            fn=process_video,
            inputs=[input_video, prompt, align_method, process_size, upscale, process_mode],
            outputs=[output_video, status_text, info_text]
        )
    
    return demo

if __name__ == "__main__":
    # åˆ›å»ºWebç•Œé¢
    demo = create_web_interface()
    
    # å¯åŠ¨åº”ç”¨
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    ) 