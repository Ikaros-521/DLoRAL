import os
import sys
import time
import tempfile
import shutil
import gradio as gr
import torch
import cv2
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.getcwd())

# å¯¼å…¥DLoRALæ¨¡å‹
from src.DLoRAL_model import Generator_eval
from src.my_utils.wavelet_color_fix import adain_color_fix, wavelet_color_fix
from ram.models.ram_lora import ram
from ram import inference_ram as inference

# è®¾ç½®PILæœ€å¤§å›¾åƒåƒç´ 
PIL.Image.MAX_IMAGE_PIXELS = 933120000

# å…¨å±€å˜é‡
model = None
DAPE = None
weight_dtype = torch.float32

# å˜æ¢å™¨
tensor_transforms = transforms.Compose([
    transforms.ToTensor(),
])

ram_transforms = transforms.Compose([
    transforms.Resize((384, 384)),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

def initialize_models():
    """åˆå§‹åŒ–æ¨¡å‹"""
    global model, DAPE, weight_dtype
    
    print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    
    # æ¨¡å‹é…ç½®
    class ModelConfig:
        def __init__(self):
            self.pretrained_model_path = "stabilityai/stable-diffusion-2-1-base"
            self.pretrained_model_name_or_path = "stabilityai/stable-diffusion-2-1-base"
            self.ram_path = "preset/models/ram_swin_large_14m.pth"
            self.ram_ft_path = "preset/models/DAPE.pth"
            self.pretrained_path = "preset/models/checkpoints/model.pkl"
            self.process_size = 512
            self.upscale = 4
            self.align_method = "adain"
            self.vae_decoder_tiled_size = 224
            self.vae_encoder_tiled_size = 4096
            self.latent_tiled_size = 96
            self.latent_tiled_overlap = 32
            self.mixed_precision = "fp16"
            self.merge_and_unload_lora = False
            self.stages = 1
            self.load_cfr = True
            self.prompt = ""
            self.save_prompts = False
    
    args = ModelConfig()
    
    # åˆå§‹åŒ–DLoRALæ¨¡å‹
    model = Generator_eval(args)
    model.set_eval()
    
    # åˆå§‹åŒ–RAMæ¨¡å‹
    DAPE = ram(pretrained=args.ram_path,
               pretrained_condition=args.ram_ft_path,
               image_size=384,
               vit='swin_l')
    DAPE.eval()
    DAPE.to("cuda")
    
    # è®¾ç½®æƒé‡ç±»å‹
    if args.mixed_precision == "fp16":
        weight_dtype = torch.float16
    elif args.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16
    
    # è®¾ç½®æ¨¡å‹æƒé‡ç±»å‹
    DAPE = DAPE.to(dtype=weight_dtype)
    model.vae = model.vae.to(dtype=weight_dtype)
    model.unet = model.unet.to(dtype=weight_dtype)
    model.cfr_main_net = model.cfr_main_net.to(dtype=weight_dtype)
    
    # è®¾ç½®adapter
    if args.stages == 0:
        model.unet.set_adapter(['default_encoder_consistency', 'default_decoder_consistency', 'default_others_consistency'])
    else:
        model.unet.set_adapter(['default_encoder_quality', 'default_decoder_quality',
                                'default_others_quality',
                                'default_encoder_consistency', 'default_decoder_consistency',
                                'default_others_consistency'])
    
    print("æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼")

def get_validation_prompt(image, prompt=""):
    """è·å–éªŒè¯æç¤ºè¯"""
    global DAPE, weight_dtype
    
    validation_prompt = ""
    lq = tensor_transforms(image).unsqueeze(0).to("cuda")
    lq = ram_transforms(lq).to(dtype=weight_dtype)
    captions = inference(lq, DAPE)
    validation_prompt = f"{captions[0]}, {prompt},"
    
    return validation_prompt

def extract_frames(video_path, output_dir):
    """æå–è§†é¢‘å¸§"""
    video_capture = cv2.VideoCapture(video_path)
    
    frame_number = 0
    success, frame = video_capture.read()
    frame_images = []
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # å¾ªç¯å¤„ç†å¸§
    while success:
        frame_filename = f"frame_{frame_number:04d}.png"
        frame_path = os.path.join(output_dir, frame_filename)
        cv2.imwrite(frame_path, frame)
        frame_images.append(frame_path)
        
        success, frame = video_capture.read()
        frame_number += 1
    
    video_capture.release()
    print(f"ä» {video_path} æå–äº† {len(frame_images)} å¸§")
    
    return frame_images

def compute_frame_difference_mask(frames):
    """è®¡ç®—å¸§å·®å¼‚æ©ç """
    ambi_matrix = frames.var(dim=0)
    threshold = ambi_matrix.mean().item()
    mask_id = torch.where(ambi_matrix >= threshold, ambi_matrix, torch.zeros_like(ambi_matrix))
    frame_mask = torch.where(mask_id == 0, mask_id, torch.ones_like(mask_id))
    return frame_mask

def process_video_super_resolution(video_path, prompt="", align_method="adain", process_size=512, upscale=4):
    """å¤„ç†è§†é¢‘è¶…åˆ†è¾¨ç‡"""
    global model, weight_dtype
    
    if model is None:
        return None, "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆå§‹åŒ–æ¨¡å‹"
    
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            frames_dir = os.path.join(temp_dir, "frames")
            output_dir = os.path.join(temp_dir, "output")
            os.makedirs(frames_dir, exist_ok=True)
            os.makedirs(output_dir, exist_ok=True)
            
            # æå–å¸§
            frame_images = extract_frames(video_path, frames_dir)
            
            if len(frame_images) == 0:
                return None, "æ— æ³•ä»è§†é¢‘ä¸­æå–å¸§"
            
            # å¤„ç†å‚æ•°
            frame_num = 2
            frame_overlap = 1
            
            # åˆå§‹åŒ–æ‰¹æ¬¡
            input_image_batch = []
            input_image_gray_batch = []
            bname_batch = []
            exist_prompt = 0
            validation_prompt = ""
            
            # å¤„ç†æ¯ä¸€å¸§
            for image_name in frame_images:
                input_image = Image.open(image_name).convert('RGB')
                input_image_gray = input_image.convert('L')
                ori_width, ori_height = input_image.size
                rscale = upscale
                resize_flag = False
                
                # å¦‚æœå›¾åƒå°äºæ‰€éœ€å°ºå¯¸ï¼Œè¿›è¡Œç¼©æ”¾
                if ori_width < process_size // rscale or ori_height < process_size // rscale:
                    scale = (process_size // rscale) / min(ori_width, ori_height)
                    input_image = input_image.resize((int(scale * ori_width), int(scale * ori_height)))
                    input_image_gray = input_image_gray.resize((int(scale * ori_width), int(scale * ori_height)))
                    resize_flag = True
                
                # æŒ‰è¶…åˆ†å€æ•°æ”¾å¤§å›¾åƒå°ºå¯¸
                input_image = input_image.resize((input_image.size[0] * rscale, input_image.size[1] * rscale))
                input_image_gray = input_image_gray.resize((input_image_gray.size[0] * rscale, input_image_gray.size[1] * rscale))
                
                # è°ƒæ•´å›¾åƒå°ºå¯¸ç¡®ä¿æ˜¯8çš„å€æ•°
                new_width = input_image.width - input_image.width % 8
                new_height = input_image.height - input_image.height % 8
                input_image = input_image.resize((new_width, new_height), Image.LANCZOS)
                input_image_gray = input_image_gray.resize((new_width, new_height), Image.LANCZOS)
                
                bname = os.path.basename(image_name)
                bname_batch.append(bname)
                
                # ç”Ÿæˆæç¤ºè¯
                if exist_prompt == 0:
                    validation_prompt = get_validation_prompt(input_image, prompt)
                    exist_prompt = 1
                
                input_image_batch.append(input_image)
                input_image_gray_batch.append(input_image_gray)
            
            # å¤„ç†å¸§æ‰¹æ¬¡
            processed_frames = []
            
            for input_image_index in range(0, len(input_image_batch), (frame_num - frame_overlap)):
                if input_image_index + frame_num - 1 >= len(input_image_batch):
                    end = len(input_image_batch) - input_image_index
                    start = 0
                else:
                    start = 0
                    end = frame_num
                
                # æ”¶é›†è¦å¤„ç†çš„å¸§æ‰¹æ¬¡
                input_frames = []
                input_frames_gray = []
                
                for input_frame_index in range(start, end):
                    real_idx = input_image_index + input_frame_index
                    if real_idx < 0 or real_idx >= len(input_image_batch):
                        continue
                    
                    current_frame = transforms.functional.to_tensor(input_image_batch[real_idx])
                    current_frame_gray = transforms.functional.to_tensor(input_image_gray_batch[real_idx])
                    current_frame_gray = torch.nn.functional.interpolate(current_frame_gray.unsqueeze(0), scale_factor=0.125).squeeze(0)
                    input_frames.append(current_frame)
                    input_frames_gray.append(current_frame_gray)
                
                if len(input_frames) < 2:
                    break
                
                input_image_final = torch.stack(input_frames, dim=0)
                input_image_gray_final = torch.stack(input_frames_gray, dim=0)
                
                # è®¡ç®—ä¸ç¡®å®šæ€§å›¾
                uncertainty_map = []
                for image_index in range(input_image_final.shape[0]):
                    if image_index != 0:
                        cur_img = input_image_gray_final[image_index]
                        prev_img = input_image_gray_final[image_index - 1]
                        compute_frame = torch.stack([cur_img, prev_img])
                        uncertainty_map_each = compute_frame_difference_mask(input_image_gray_final)
                        uncertainty_map.append(uncertainty_map_each)
                
                if len(uncertainty_map) == 0:
                    continue
                
                uncertainty_map = torch.stack(uncertainty_map)
                
                # æ¨¡å‹æ¨ç†
                with torch.no_grad():
                    c_t = input_image_final.unsqueeze(0).cuda() * 2 - 1
                    c_t = c_t.to(dtype=weight_dtype)
                    output_image, _, _, _, _ = model(stages=1, c_t=c_t, 
                                                   uncertainty_map=uncertainty_map.unsqueeze(0).cuda(), 
                                                   prompt=validation_prompt, 
                                                   weight_dtype=weight_dtype)
                
                frame_t = output_image[0]
                frame_t = (frame_t.cpu() * 0.5 + 0.5)
                output_pil = transforms.ToPILImage()(frame_t)
                
                # é¢œè‰²æ ¡æ­£
                src_idx = input_image_index + start + 1
                if src_idx < 0 or src_idx >= len(input_image_batch):
                    src_idx = max(0, min(src_idx, len(input_image_batch) - 1))
                
                source_pil = input_image_batch[src_idx]
                
                if align_method == 'adain':
                    output_pil = adain_color_fix(target=output_pil, source=source_pil)
                elif align_method == 'wavelet':
                    output_pil = wavelet_color_fix(target=output_pil, source=source_pil)
                
                # å¦‚æœä¹‹å‰è¿›è¡Œäº†ç¼©æ”¾ï¼Œç°åœ¨æ¢å¤åŸå§‹å°ºå¯¸
                if resize_flag:
                    new_w = int(upscale * ori_width)
                    new_h = int(upscale * ori_height)
                    output_pil = output_pil.resize((new_w, new_h), Image.BICUBIC)
                
                processed_frames.append(output_pil)
            
            # ä¿å­˜å¤„ç†åçš„å¸§
            output_frames = []
            for i, frame in enumerate(processed_frames):
                frame_path = os.path.join(output_dir, f"processed_frame_{i:04d}.png")
                frame.save(frame_path)
                output_frames.append(frame_path)
            
            # åˆ›å»ºè¾“å‡ºè§†é¢‘
            if len(output_frames) > 0:
                # è¯»å–ç¬¬ä¸€å¸§è·å–å°ºå¯¸
                first_frame = cv2.imread(output_frames[0])
                height, width, layers = first_frame.shape
                
                # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
                output_video_path = os.path.join(temp_dir, "output_video.mp4")
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                video_writer = cv2.VideoWriter(output_video_path, fourcc, 30.0, (width, height))
                
                # å†™å…¥å¸§
                for frame_path in output_frames:
                    frame = cv2.imread(frame_path)
                    video_writer.write(frame)
                
                video_writer.release()
                
                # å¤åˆ¶åˆ°æœ€ç»ˆè¾“å‡ºè·¯å¾„
                final_output_path = os.path.join("results", f"output_{int(time.time())}.mp4")
                os.makedirs("results", exist_ok=True)
                shutil.copy2(output_video_path, final_output_path)
                
                return final_output_path, f"å¤„ç†å®Œæˆï¼å…±å¤„ç†äº† {len(processed_frames)} å¸§"
            else:
                return None, "æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å¸§"
                
    except Exception as e:
        return None, f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"

def create_web_interface():
    """åˆ›å»ºWebç•Œé¢"""
    
    # è‡ªå®šä¹‰CSSæ ·å¼
    css = """
    .gradio-container {
        max-width: 1200px !important;
    }
    .title {
        text-align: center;
        font-size: 2.5em;
        font-weight: bold;
        margin-bottom: 20px;
        color: #2c3e50;
    }
    .subtitle {
        text-align: center;
        font-size: 1.2em;
        color: #7f8c8d;
        margin-bottom: 30px;
    }
    """
    
    with gr.Blocks(css=css, title="DLoRAL è§†é¢‘è¶…åˆ†è¾¨ç‡") as demo:
        gr.HTML("""
        <div class="title">ğŸ¬ DLoRAL è§†é¢‘è¶…åˆ†è¾¨ç‡</div>
        <div class="subtitle">ä¸€æ­¥æ‰©æ•£ï¼šç»†èŠ‚ä¸°å¯Œä¸”æ—¶é—´ä¸€è‡´çš„è§†é¢‘è¶…åˆ†è¾¨ç‡</div>
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¤ è¾“å…¥è®¾ç½®")
                
                # æ¨¡å‹åˆå§‹åŒ–æŒ‰é’®
                init_button = gr.Button("ğŸš€ åˆå§‹åŒ–æ¨¡å‹", variant="primary", size="lg")
                init_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", value="æ¨¡å‹æœªåˆå§‹åŒ–", interactive=False)
                
                # è¾“å…¥è§†é¢‘
                input_video = gr.Video(label="ä¸Šä¼ è§†é¢‘æ–‡ä»¶", type="filepath")
                
                # å‚æ•°è®¾ç½®
                with gr.Accordion("âš™ï¸ é«˜çº§å‚æ•°", open=False):
                    prompt = gr.Textbox(label="æç¤ºè¯", value="", placeholder="è¾“å…¥é¢å¤–çš„æç¤ºè¯ï¼ˆå¯é€‰ï¼‰")
                    align_method = gr.Dropdown(
                        choices=["adain", "wavelet", "nofix"], 
                        value="adain", 
                        label="é¢œè‰²æ ¡æ­£æ–¹æ³•"
                    )
                    process_size = gr.Slider(
                        minimum=256, maximum=1024, value=512, step=64,
                        label="å¤„ç†å°ºå¯¸"
                    )
                    upscale = gr.Slider(
                        minimum=2, maximum=8, value=4, step=1,
                        label="è¶…åˆ†å€æ•°"
                    )
                
                # å¤„ç†æŒ‰é’®
                process_button = gr.Button("ğŸ¯ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¥ è¾“å‡ºç»“æœ")
                
                # è¾“å‡ºè§†é¢‘
                output_video = gr.Video(label="å¤„ç†åçš„è§†é¢‘")
                
                # å¤„ç†çŠ¶æ€
                status_text = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False)
                
                # å¤„ç†ä¿¡æ¯
                info_text = gr.Textbox(label="å¤„ç†ä¿¡æ¯", interactive=False, lines=3)
        
        # ç¤ºä¾‹
        gr.Markdown("### ğŸ“‹ ä½¿ç”¨è¯´æ˜")
        gr.Markdown("""
        1. **åˆå§‹åŒ–æ¨¡å‹**: é¦–æ¬¡ä½¿ç”¨æˆ–é‡å¯åéœ€è¦å…ˆåˆå§‹åŒ–æ¨¡å‹
        2. **ä¸Šä¼ è§†é¢‘**: æ”¯æŒMP4æ ¼å¼çš„è§†é¢‘æ–‡ä»¶
        3. **è®¾ç½®å‚æ•°**: å¯é€‰æ‹©è°ƒæ•´å¤„ç†å‚æ•°
        4. **å¼€å§‹å¤„ç†**: ç‚¹å‡»å¤„ç†æŒ‰é’®å¼€å§‹è§†é¢‘è¶…åˆ†è¾¨ç‡
        5. **ä¸‹è½½ç»“æœ**: å¤„ç†å®Œæˆåå¯ä¸‹è½½å¢å¼ºåçš„è§†é¢‘
        
        **æ³¨æ„äº‹é¡¹**:
        - å¤„ç†æ—¶é—´å–å†³äºè§†é¢‘é•¿åº¦å’Œåˆ†è¾¨ç‡
        - å»ºè®®è§†é¢‘åˆ†è¾¨ç‡ä¸è¶…è¿‡1080p
        - ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUå†…å­˜
        """)
        
        # äº‹ä»¶å¤„ç†
        def init_models():
            try:
                initialize_models()
                return "âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼"
            except Exception as e:
                return f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}"
        
        def process_video(video_path, prompt, align_method, process_size, upscale):
            if video_path is None:
                return None, "è¯·å…ˆä¸Šä¼ è§†é¢‘æ–‡ä»¶", "æœªå¤„ç†"
            
            if model is None:
                return None, "è¯·å…ˆåˆå§‹åŒ–æ¨¡å‹", "æ¨¡å‹æœªåˆå§‹åŒ–"
            
            output_path, message = process_video_super_resolution(
                video_path, prompt, align_method, process_size, upscale
            )
            
            return output_path, message, f"å¤„ç†å®Œæˆ - {message}"
        
        # ç»‘å®šäº‹ä»¶
        init_button.click(
            fn=init_models,
            outputs=init_status
        )
        
        process_button.click(
            fn=process_video,
            inputs=[input_video, prompt, align_method, process_size, upscale],
            outputs=[output_video, status_text, info_text]
        )
    
    return demo

if __name__ == "__main__":
    # åˆ›å»ºWebç•Œé¢
    demo = create_web_interface()
    
    # å¯åŠ¨åº”ç”¨
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    ) 